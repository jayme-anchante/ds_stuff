## General

* [BTB](https://github.com/HDI-Project/BTB) - Bayesian tuning and bandits: by MIT
* [Chocolate](https://github.com/AIworx-Labs/chocolate): A fully decentralized hyperparameter optimization framework. Asynchonous framework relying on database (SQLite, MongoDB, pandas.DataFrame) to share information between workers with no master to distribute tasks, implements Bayesian and Multi-armed bandit problem optimizers
* [GPyOpt](https://github.com/SheffieldML/GPyOpt): Gaussian Process Optimization using GPy
* [HpBandSter](https://github.com/automl/HpBandSter): a distributed Hyperband implementation on Steroids. Implements Bayesian Optimization and Hyperband (BOHB), can access results before finishing the experiment.
* [HPOlib](https://github.com/automl/HPOlib) (UNMAINTAINED): HPOlib is a hyperparameter optimization library. It provides a common interface to three state of the art hyperparameter optimization packages: SMAC, spearmint and hyperopt.
* [Hyperband](https://github.com/zygmuntz/hyperband): Tuning hyperparams fast with Hyperband
* [Hyperopt](https://github.com/hyperopt/hyperopt): Distributed Asynchronous Hyperparameter Optimization in Python. Supports integer, float and categorical variables, can't see results before finishing, distributed optimization is done via MongoDB, implements the Tree-structed Panzen Estimator.
* [HyperparameterHunter](https://github.com/HunterMcGushion/hyperparameter_hunter): Easy hyperparameter optimization and automatic result saving across machine learning algorithms and libraries.
* [Milano](https://github.com/NVIDIA/Milano) - Machine learning autotuner and network optimizer by Nvidia: a tool for enabling machine learning researchers and practitioners to perform massive hyperparameters and architecture searches.
* [NNI](https://github.com/Microsoft/nni) - Neural Network Inteligence: An open source AutoML toolkit for neural architecture search and hyper-parameter tuning.
* [Optuna](https://github.com/pfnet/optuna): A hyperparameter optimization framework.
* [Orion](https://github.com/Epistimio/orion): Asynchronous Distributed Hyperparameter Optimization.
* [ray](https://github.com/ray-project/ray): A system for parallel and distributed Python that unifies the ML ecosystem. Comes with [Tune](http://ray.readthedocs.io/en/latest/tune.html) (Hyperparameter Optimization Framework), [RLib](http://ray.readthedocs.io/en/latest/rllib.html) (Scalable Reinforcement Learning) and [Distributed Training](http://ray.readthedocs.io/en/latest/distributed_sgd.html), implements [Bayes](https://github.com/fmfn/BayesianOptimization), [Hypeopt](http://hyperopt.github.io/hyperopt)'s TPE, [SigOptSearch](https://sigopt.com/), [Nevergrad](https://github.com/facebookresearch/nevergrad) and [Scikit-Optimize](https://scikit-optimize.github.io/).
* [RoBo](https://github.com/automl/RoBO) - Robust Bayesian Optimization: implements Gaussian Processes, Bohamiann, Fabolas
* [SHERPA](https://github.com/sherpa-ai/sherpa): Hyperparameter optimization that enables researchers to experiment, visualize, and scale quickly. Implements Local Search, GPyOpt Bayesian Optimization and Population Based Training.
* [SMAC3](https://github.com/automl/SMAC3): Sequential Model-based Algorithm Configuration
* [Spearmint](https://github.com/HIPS/Spearmint): Spearmint Bayesian optimization codebase. Academic and non-comercial research use license.
* [Test Tube](https://github.com/williamFalcon/test-tube): Python library to easily log experiments and parallelize hyperparameter search for neural networks. Interface with tensorflow, keras, pytorch, caffe, caffe2, chainer, mxnet, theano, sciki-learn
* [Tune](https://ray.readthedocs.io/en/latest/tune.html): Scalable Hyperparameter Search

## Academic

* [HORD](https://github.com/ilija139/HORD): Efficient Hyperparameter Optimization of Deep Learning Algorithms Using Deterministic RBF Surrogates. Code for reproducing the [paper](https://arxiv.org/abs/1607.08316)
* [Cornell-MOE](https://github.com/wujian16/Cornell-MOE): A Python library for the state-of-the-art parallel Bayesian optimization algorithms, with the core implemented in C++. Implements BayesOpt

## Evolutionary algorithms

* [deap](https://github.com/DEAP/deap): Distributed Evolutionary Algorithms in Python
* [DEvol](https://github.com/joeddav/devol): Genetic ConvNet architecture search with Keras
* [

## Keras specific

* [DeepReplay](https://github.com/dvgodoy/deepreplay)
* [Hyperas](https://github.com/maxpumperla/hyperas): Keras Wrapper for Hyperopt
* [Kopt](https://github.com/Avsecz/kopt): Hyperopt Based Optimizer
* [Talos](https://github.com/autonomio/talos): Hyperparameter Optimization for Keras Models

## PyTorch

* [Hypersearch](https://github.com/kevinzakka/hypersearch): Hyperparameter Optimization for PyTorch
